"""
Advanced ML Training for Predictive Maintenance
Topic: "Predictive Maintenance Strategies Using Big Data And Machine Learning"

Features:
- Advanced feature engineering
- Multiple ML algorithms optimized for 90%+ accuracy
- Big Data handling with efficient processing
- Comprehensive model evaluation
"""
import json
import os
import argparse
import joblib
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import GroupShuffleSplit, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    f1_score, roc_auc_score, average_precision_score, 
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score
)
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from xgboost import XGBClassifier

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except Exception:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è  LightGBM not available")

from catboost import CatBoostClassifier

MODELS_DIR = os.path.join(os.path.dirname(__file__), "../backend/models")
os.makedirs(MODELS_DIR, exist_ok=True)

def engineer_features(df):
    """
    Advanced feature engineering for predictive maintenance.
    Creates interaction features and derived metrics for better accuracy.
    """
    print("\nüîß Engineering advanced features for Big Data ML...")
    
    df_eng = df.copy()
    
    # If engineered features already exist, return
    if 'temp_avg' in df_eng.columns:
        print("   ‚úì Engineered features detected")
        return df_eng
    
    # Temperature features
    temp_cols = [c for c in df.columns if c.startswith('s') and int(c[1:]) <= 4]
    if temp_cols:
        df_eng['temp_avg'] = df_eng[temp_cols].mean(axis=1)
        df_eng['temp_std'] = df_eng[temp_cols].std(axis=1)
        df_eng['temp_max'] = df_eng[temp_cols].max(axis=1)
        df_eng['temp_range'] = df_eng[temp_cols].max(axis=1) - df_eng[temp_cols].min(axis=1)
    
    # Pressure features
    pressure_cols = [c for c in df.columns if c.startswith('s') and 5 <= int(c[1:]) <= 8]
    if pressure_cols:
        df_eng['pressure_avg'] = df_eng[pressure_cols].mean(axis=1)
        df_eng['pressure_std'] = df_eng[pressure_cols].std(axis=1)
        df_eng['pressure_min'] = df_eng[pressure_cols].min(axis=1)
    
    # Vibration features
    vib_cols = [c for c in df.columns if c.startswith('s') and 9 <= int(c[1:]) <= 12]
    if vib_cols:
        df_eng['vibration_avg'] = df_eng[vib_cols].mean(axis=1)
        df_eng['vibration_max'] = df_eng[vib_cols].max(axis=1)
        df_eng['vibration_std'] = df_eng[vib_cols].std(axis=1)
    
    # RPM/Flow features
    rpm_cols = [c for c in df.columns if c.startswith('s') and 13 <= int(c[1:]) <= 16]
    if rpm_cols:
        df_eng['rpm_avg'] = df_eng[rpm_cols].mean(axis=1)
        df_eng['rpm_std'] = df_eng[rpm_cols].std(axis=1)
    
    # Interaction features (critical for 90%+ accuracy)
    if 'temp_avg' in df_eng.columns and 'vibration_avg' in df_eng.columns:
        df_eng['temp_vib_interaction'] = df_eng['temp_avg'] * df_eng['vibration_avg']
    
    if 'pressure_avg' in df_eng.columns and 'rpm_avg' in df_eng.columns:
        df_eng['pressure_rpm_ratio'] = df_eng['pressure_avg'] / (df_eng['rpm_avg'] + 1)
    
    # Cycle-based features (temporal patterns)
    if 'cycle' in df_eng.columns:
        df_eng['cycle_squared'] = df_eng['cycle'] ** 2
        df_eng['cycle_sqrt'] = np.sqrt(df_eng['cycle'])
    
    print(f"   ‚úì Created {len([c for c in df_eng.columns if c not in df.columns])} new features")
    
    return df_eng

def create_optimized_pipelines():
    """
    Create highly optimized ML pipelines for 90%+ accuracy.
    Uses extensive hyperparameter tuning results.
    """
    print("\nü§ñ Creating optimized ML pipelines...")
    
    pipelines = {
        "random_forest": Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
            ("scale", RobustScaler()),  # Better for outliers
            ("rf", RandomForestClassifier(
                n_estimators=800,           # Increased trees
                max_depth=25,               # Deeper trees
                min_samples_split=3,        # Lower threshold
                min_samples_leaf=1,         # More granular
                max_features='sqrt',        # Optimal for classification
                class_weight='balanced',
                bootstrap=True,
                random_state=42,
                n_jobs=-1
            ))
        ]),
        
        "xgboost": Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
            ("scale", RobustScaler()),
            ("xgb", XGBClassifier(
                n_estimators=1000,          # More iterations
                max_depth=8,                # Optimized depth
                learning_rate=0.03,         # Lower for better convergence
                subsample=0.85,
                colsample_bytree=0.85,
                gamma=0.1,                  # Regularization
                min_child_weight=1,
                scale_pos_weight=2.0,       # Handle imbalance
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss',
                tree_method='hist'          # Faster for big data
            ))
        ]),
        
        "gradient_boosting": Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
            ("scale", RobustScaler()),
            ("gb", GradientBoostingClassifier(
                n_estimators=500,
                max_depth=7,
                learning_rate=0.05,
                subsample=0.8,
                max_features='sqrt',
                random_state=42
            ))
        ]),
        
        "catboost": Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
            ("scale", RobustScaler()),
            ("cat", CatBoostClassifier(
                iterations=1000,            # More iterations
                depth=8,                    # Optimized depth
                learning_rate=0.03,         # Lower learning rate
                l2_leaf_reg=5,              # Regularization
                border_count=128,           # Higher precision
                auto_class_weights='Balanced',
                random_seed=42,
                verbose=False,
                thread_count=-1
            ))
        ])
    }
    
    # Add LightGBM if available
    if LIGHTGBM_AVAILABLE:
        pipelines["lightgbm"] = Pipeline(steps=[
            ("impute", SimpleImputer(strategy="median")),
            ("scale", RobustScaler()),
            ("lgbm", LGBMClassifier(
                n_estimators=1000,
                max_depth=12,
                learning_rate=0.03,
                num_leaves=50,
                subsample=0.85,
                colsample_bytree=0.85,
                min_child_samples=20,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1,
                verbose=-1
            ))
        ])
    
    print(f"   ‚úì Created {len(pipelines)} optimized models")
    return pipelines

def train_and_evaluate(pipe, Xtr, ytr, Xte, yte, model_name):
    """
    Train a model with comprehensive evaluation metrics.
    """
    print(f"\n{'='*70}")
    print(f"üéØ Training: {model_name}")
    print(f"{'='*70}")
    
    # Train
    pipe.fit(Xtr, ytr)
    
    # Predictions
    proba = pipe.predict_proba(Xte)[:, 1]
    pred = (proba >= 0.5).astype(int)
    
    # Comprehensive metrics
    accuracy = accuracy_score(yte, pred)
    f1 = f1_score(yte, pred)
    auroc = roc_auc_score(yte, proba)
    ap = average_precision_score(yte, proba)
    precision = precision_score(yte, pred)
    recall = recall_score(yte, pred)
    
    # Confusion matrix
    cm = confusion_matrix(yte, pred)
    tn, fp, fn, tp = cm.ravel()
    
    metrics = {
        "model_name": model_name,
        "accuracy": float(accuracy),
        "f1": float(f1),
        "auroc": float(auroc),
        "average_precision": float(ap),
        "precision": float(precision),
        "recall": float(recall),
        "true_negatives": int(tn),
        "false_positives": int(fp),
        "false_negatives": int(fn),
        "true_positives": int(tp),
        "n_train": int(len(Xtr)),
        "n_test": int(len(Xte)),
        "positive_rate_train": float(ytr.mean()),
        "positive_rate_test": float(yte.mean())
    }
    
    # Print results
    print(f"\nüìä Results:")
    print(f"   üéØ Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   üìà F1 Score:  {f1:.4f}")
    print(f"   üìâ AUROC:     {auroc:.4f}")
    print(f"   ‚öñÔ∏è  Precision: {precision:.4f}")
    print(f"   üîç Recall:    {recall:.4f}")
    print(f"\n   Confusion Matrix:")
    print(f"      TN: {tn:5d}  |  FP: {fp:5d}")
    print(f"      FN: {fn:5d}  |  TP: {tp:5d}")
    
    if accuracy >= 0.90:
        print(f"   üèÜ TARGET ACHIEVED: {accuracy*100:.2f}% ‚â• 90%")
    
    return pipe, metrics

def main():
    parser = argparse.ArgumentParser(description="Train ML models for predictive maintenance")
    parser.add_argument("--csv", required=True, help="Path to training CSV")
    parser.add_argument("--label", default="label", help="Label column name")
    parser.add_argument("--group", default="unit", help="Group column for splitting")
    parser.add_argument("--models", nargs="+", help="Specific models to train")
    
    args = parser.parse_args()
    
    print("="*70)
    print("  PREDICTIVE MAINTENANCE ML TRAINING")
    print("  Big Data & Machine Learning Strategies")
    print("="*70)
    
    # Load data
    print(f"\nüìÇ Loading data from: {args.csv}")
    df = pd.read_csv(args.csv)
    print(f"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
    
    # Engineer features
    df = engineer_features(df)
    print(f"   Enhanced shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
    
    # Prepare features
    label_col = args.label
    exclude_cols = [label_col, args.group, 'RUL', 'timestamp', 'failure_mode']
    features = [c for c in df.columns if c not in exclude_cols]
    
    X = df[features]
    y = df[label_col].values
    groups = df[args.group].values if args.group in df.columns else None
    
    print(f"\nüìä Data Statistics:")
    print(f"   Features: {len(features)}")
    print(f"   Samples: {len(X):,}")
    print(f"   Positive class: {y.sum():,} ({y.mean()*100:.1f}%)")
    print(f"   Negative class: {(1-y).sum():,} ({(1-y).mean()*100:.1f}%)")
    
    # Split data
    if groups is not None:
        print(f"\n‚úÇÔ∏è  Splitting with GroupShuffleSplit (by '{args.group}')")
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        train_idx, test_idx = next(gss.split(X, y, groups))
    else:
        print(f"\n‚úÇÔ∏è  Splitting with stratified split (80/20)")
        from sklearn.model_selection import train_test_split
        train_idx, test_idx = train_test_split(
            range(len(X)), test_size=0.2, stratify=y, random_state=42
        )
    
    Xtr, Xte = X.iloc[train_idx], X.iloc[test_idx]
    ytr, yte = y[train_idx], y[test_idx]
    
    print(f"   Train: {len(Xtr):,} samples")
    print(f"   Test:  {len(Xte):,} samples")
    
    # Create pipelines
    all_pipelines = create_optimized_pipelines()
    
    # Filter models
    if args.models:
        pipelines = {k: v for k, v in all_pipelines.items() if k in args.models}
    else:
        pipelines = all_pipelines
    
    print(f"\nüöÄ Training {len(pipelines)} models: {list(pipelines.keys())}")
    
    # Train all models
    trained_models = {}
    all_metrics = []
    
    for model_name, pipe in pipelines.items():
        trained_pipe, metrics = train_and_evaluate(pipe, Xtr, ytr, Xte, yte, model_name)
        trained_models[model_name] = trained_pipe
        all_metrics.append(metrics)
    
    # Find best model
    best_model = max(all_metrics, key=lambda x: x["accuracy"])
    
    print(f"\n{'='*70}")
    print(f"üèÜ BEST MODEL: {best_model['model_name'].upper()}")
    print(f"   Accuracy: {best_model['accuracy']:.4f} ({best_model['accuracy']*100:.2f}%)")
    print(f"   F1 Score: {best_model['f1']:.4f}")
    print(f"   AUROC:    {best_model['auroc']:.4f}")
    print(f"{'='*70}")
    
    # Count models achieving 90%+
    high_accuracy_models = [m for m in all_metrics if m['accuracy'] >= 0.90]
    if high_accuracy_models:
        print(f"\nüéâ {len(high_accuracy_models)} model(s) achieved 90%+ accuracy!")
        for m in high_accuracy_models:
            print(f"   ‚úì {m['model_name']}: {m['accuracy']*100:.2f}%")
    
    # Save models
    print(f"\nüíæ Saving models to: {MODELS_DIR}")
    for model_name, pipe in trained_models.items():
        model_path = os.path.join(MODELS_DIR, f"{model_name}.pkl")
        model_metrics = next(m for m in all_metrics if m["model_name"] == model_name)
        joblib.dump({
            "pipeline": pipe,
            "features": features,
            "metrics": model_metrics
        }, model_path)
        print(f"   ‚úÖ {model_name}.pkl")
    
    # Save best model as default
    default_path = os.path.join(MODELS_DIR, "model.pkl")
    best_pipe = trained_models[best_model["model_name"]]
    joblib.dump({
        "pipeline": best_pipe,
        "features": features,
        "metrics": best_model
    }, default_path)
    print(f"   ‚úÖ model.pkl (best: {best_model['model_name']})")
    
    # Save metadata
    with open(os.path.join(MODELS_DIR, "feature_list.json"), "w") as f:
        json.dump(features, f, indent=2)
    
    with open(os.path.join(MODELS_DIR, "all_metrics.json"), "w") as f:
        json.dump(all_metrics, f, indent=2)
    
    summary_df = pd.DataFrame(all_metrics).sort_values("accuracy", ascending=False)
    summary_df.to_csv(os.path.join(MODELS_DIR, "model_comparison.csv"), index=False)
    
    print(f"\n{'='*70}")
    print("‚ú® TRAINING COMPLETE")
    print(f"{'='*70}")
    print("\nüìä Model Comparison (sorted by accuracy):")
    print(summary_df[['model_name', 'accuracy', 'f1', 'auroc', 'precision', 'recall']].to_string(index=False))
    print(f"\nüéØ Project: Predictive Maintenance Strategies Using Big Data And ML")
    print(f"‚úÖ Models ready for deployment!")

if __name__ == "__main__":
    main()
